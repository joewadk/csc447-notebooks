{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GQmLq5G1p9F"
      },
      "source": [
        "## Natural Language Processing (NLP)\n",
        "\n",
        "- Scikit-learn excels at transforming text data into numerical representations that machine learning algorithms can process.This is achieved through modules like sklearn.feature_extraction.text\n",
        "\n",
        "- NLTK (Natural Language Toolkit): A comprehensive suite for symbolic and statistical NLP, offering tools for tokenization, stemming, tagging, parsing, and semantic reasoning, ideal for educational and research purposes.\n",
        "\n",
        "- spaCy: A fast and efficient library designed for production-level NLP tasks, providing robust features like tokenization, part-of-speech tagging, named entity recognition, and dependency parsing with pre-trained models.\n",
        "\n",
        "- Gensim: Focused on topic modeling and document similarity, Gensim excels at tasks like Latent Dirichlet Allocation (LDA) and word embeddings, making it suitable for analyzing large text corpora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD2cvTzr1l2H"
      },
      "source": [
        "### CountVectorizer\n",
        "Converts text documents into a matrix of word counts. This is often referred to as the \"bag-of-words\" model.\n",
        "Some common nlp steps are included:\n",
        "\n",
        "- Tokenization: token_pattern, regex.\n",
        "- Normalization: lowercase, punctuation removal (implicit).\n",
        "- Stop Words: stop_words parameter.\n",
        "- N-grams: ngram_range.\n",
        "- Vocabulary: unique tokens.\n",
        "- Document-Term Matrix: frequency counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5onruLrD0frR",
        "outputId": "d60a4595-9702-4e19-f371-f996a8dd0342"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MibEgFoX3Vo-",
        "outputId": "88e66c2e-0923-430b-fa30-aca743cc0e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "print(X.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_At6Esvz6INd",
        "outputId": "e86595d5-1b84-4865-f9df-6cf17aae0a92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['document', 'second'], dtype=object)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# stop words removal\n",
        "vectorizer2 = CountVectorizer(stop_words='english')\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "vectorizer2.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti5B16HA6lNl",
        "outputId": "d96356f5-05c8-47d5-a274-4c2291d3ff15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 0]\n",
            " [2 1]\n",
            " [0 0]\n",
            " [1 0]]\n"
          ]
        }
      ],
      "source": [
        "print(X2.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZLTRJoT8IZs",
        "outputId": "b8380988-6d8e-4037-c9dd-909258973115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and this', 'document is', 'first document', 'is the', 'is this',\n",
              "       'second document', 'the first', 'the second', 'the third',\n",
              "       'third one', 'this document', 'this is', 'this the'], dtype=object)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Set ngram (a contiguous sequence of n items)\n",
        "vectorizer3 = CountVectorizer(ngram_range=(2, 2)) # ngram (min, max)\n",
        "X3 = vectorizer3.fit_transform(corpus)\n",
        "vectorizer3.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyQrCQg69oCa",
        "outputId": "fc05b935-28f3-4c31-9b26-6e39e55251e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
            " [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
            " [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
            " [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "print(X3.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nplDW1bM9lV7"
      },
      "source": [
        "### TfidfVectorizer\n",
        "\n",
        "TfidfVectorizer() helps to create a numerical representation of text data that emphasizes the importance of terms within a document relative to the entire corpus.\n",
        "\n",
        "Use Cases:\n",
        "\n",
        "- Text classification\n",
        "- Information retrieval\n",
        "- Sentiment analysis\n",
        "- Document clustering\n",
        "- Keyword extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ifVPCzB9vST",
        "outputId": "e557f671-f7a0-46af-c3fc-43d0f86865cf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
              "       'this'], dtype=object)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer() # stop_words = 'english' if needed\n",
        "X4 = vectorizer.fit_transform(corpus)\n",
        "vectorizer.get_feature_names_out()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hE33rGgP-Fj8",
        "outputId": "db2f7f82-8d72-4a24-c6ea-a211b94af958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]\n",
            " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
            "  0.28108867 0.         0.28108867]\n",
            " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
            "  0.26710379 0.51184851 0.26710379]\n",
            " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ]
        }
      ],
      "source": [
        "print(X4.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9XD2pSn_LB6"
      },
      "source": [
        "### NLTK\n",
        "\n",
        "NLTK offers a vast collection of algorithms and tools for various NLP tasks, including:\n",
        "- Tokenization (splitting text into words or sentences)\n",
        "- Stemming and lemmatization (reducing words to their root form)\n",
        "- Part-of-speech tagging (labeling words with their grammatical roles)\n",
        "- Parsing (analyzing sentence structure)\n",
        "- Named entity recognition (identifying people, places, organizations)\n",
        "- Sentiment analysis\n",
        "- Text classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtmuXj_Q_2Qj",
        "outputId": "3d29fc86-28ee-44dc-ff3c-f283930feae0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Download necessary NLTK resources (run once)\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8TAxy6JAtNS",
        "outputId": "2a1b1b95-dfe3-49cd-e8b0-f18379c6f6a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Matrix:\n",
            "  (0, 1)\t0.7772211620785797\n",
            "  (0, 0)\t0.6292275146695526\n",
            "  (1, 0)\t0.78722297610404\n",
            "  (1, 3)\t0.6166684570284895\n",
            "  (2, 4)\t0.7071067811865476\n",
            "  (2, 2)\t0.7071067811865476\n",
            "  (3, 1)\t0.7772211620785797\n",
            "  (3, 0)\t0.6292275146695526\n",
            "\n",
            "Feature Names:\n",
            "['document' 'first' 'one' 'second' 'third']\n",
            "\n",
            "TF-IDF Dense Array:\n",
            "[[0.62922751 0.77722116 0.         0.         0.        ]\n",
            " [0.78722298 0.         0.         0.61666846 0.        ]\n",
            " [0.         0.         0.70710678 0.         0.70710678]\n",
            " [0.62922751 0.77722116 0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text document using built-in Python tools.\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) #keep word characters and whitespace\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Preprocess the corpus\n",
        "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Vectorization using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
        "\n",
        "# Print the TF-IDF matrix (sparse matrix)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix)\n",
        "\n",
        "# Print the feature names (words)\n",
        "print(\"\\nFeature Names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# If you want to see the dense array representation, use toarray()\n",
        "print(\"\\nTF-IDF Dense Array:\")\n",
        "print(tfidf_matrix.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v5Wjwg2BwVy"
      },
      "source": [
        "### SpaCy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "lWQY4OD8CBoq"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\") # converted to 96 dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8APrrjUyCEGy",
        "outputId": "af331d55-0c04-46f1-dd73-2884c02a361d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Matrix:\n",
            "  (0, 0)\t1.0\n",
            "  (1, 0)\t0.78722297610404\n",
            "  (1, 1)\t0.6166684570284895\n",
            "  (3, 0)\t1.0\n",
            "\n",
            "Feature Names:\n",
            "['document' 'second']\n",
            "\n",
            "TF-IDF Dense Array:\n",
            "[[1.         0.        ]\n",
            " [0.78722298 0.61666846]\n",
            " [0.         0.        ]\n",
            " [1.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text document using spaCy.\"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Lowercase, remove punctuation, remove stopwords, and lemmatization.\n",
        "    tokens = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_stop]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess the corpus\n",
        "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Vectorization using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
        "\n",
        "# Print the TF-IDF matrix (sparse matrix)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix)\n",
        "\n",
        "# Print the feature names (words)\n",
        "print(\"\\nFeature Names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# If you want to see the dense array representation, use toarray()\n",
        "print(\"\\nTF-IDF Dense Array:\")\n",
        "print(tfidf_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyifDBajc7UK",
        "outputId": "7bc5c52b-5c86-45a9-ea99-3d7bfe941f77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I [-1.3939121  -0.38389102  0.11240871  0.20698646  0.7719366 ]\n",
            "enjoy [-1.0712639  -0.8785212  -0.6848391  -0.03444037 -0.6544076 ]\n",
            "coding [ 0.37274474  1.2042749  -0.09712669  0.84556305  0.15096729]\n",
            "in [ 0.7516444  -0.34233165  0.51338893 -1.3308781  -0.5497204 ]\n",
            "Python [-0.77589226 -0.38897672  0.23942587  0.5341584   0.44315192]\n",
            ". [-0.7804837  -0.6215247  -0.8710715  -0.92849153 -0.25889164]\n",
            "\n",
            "Document vector: [-0.48286048 -0.23516172 -0.1313023  -0.11785036 -0.01616064]\n"
          ]
        }
      ],
      "source": [
        "# Use SpaCy word2vec\n",
        "\n",
        "text = \"I enjoy coding in Python.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.vector[:5])  # Print the first 5 vector dimensions\n",
        "\n",
        "print(\"\\nDocument vector:\", doc.vector[:5]) # print the first 5 vector dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yKJfj28ejhg"
      },
      "source": [
        "### Topic Modeling\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0fRpReU5gvoX"
      },
      "outputs": [],
      "source": [
        "# Match the version between numpy and gensim\n",
        "# Skip this if you don't have the issue\n",
        "#!pip install --upgrade --force-reinstall numpy gensim\n",
        "#!pip install --upgrade --force-reinstall scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evn3g67je8OQ",
        "outputId": "2134ebb2-ca59-4310-9bad-6b47c242214e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: (0, '0.066*\"garden\" + 0.066*\"cat\" + 0.066*\"flowers\" + 0.065*\"mat\" + 0.065*\"sat\" + 0.065*\"dog\" + 0.065*\"played\" + 0.065*\"trees\" + 0.065*\"gardens\" + 0.065*\"beautiful\"')\n",
            "Topic 1: (1, '0.089*\"humans\" + 0.089*\"dogs\" + 0.051*\"loyal\" + 0.051*\"companions\" + 0.051*\"spring\" + 0.051*\"bloom\" + 0.051*\"summer\" + 0.051*\"love\" + 0.051*\"pets\" + 0.051*\"mouse\"')\n",
            "Topic 2: (2, '0.137*\"cats\" + 0.077*\"dogs\" + 0.077*\"nature\" + 0.077*\"known\" + 0.077*\"independent\" + 0.077*\"peacefully\" + 0.077*\"coexist\" + 0.019*\"flowers\" + 0.019*\"garden\" + 0.019*\"cat\"')\n",
            "\n",
            "Topic Distribution for test document:\n",
            "[(0, 0.8182691), (1, 0.097704984), (2, 0.084025994)]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\kabir\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"The cat sat on the mat. The dog played in the garden.\",\n",
        "    \"Cats are known for their independent nature.\",\n",
        "    \"Dogs are loyal companions to humans.\",\n",
        "    \"Gardens are beautiful places with flowers and trees.\",\n",
        "    \"Cats and dogs can coexist peacefully.\",\n",
        "    \"The cat chased a mouse in the garden.\",\n",
        "    \"Humans love their pets, both cats and dogs.\",\n",
        "    \"Flowers bloom in spring and summer.\"\n",
        "]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text document.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Preprocess the corpus\n",
        "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Create a dictionary from the processed corpus\n",
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "\n",
        "# Create a bag-of-words representation of the corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in enumerate(lda_model.print_topics(num_topics=-1)):\n",
        "    print('Topic {}: {}'.format(idx, topic))\n",
        "\n",
        "# Get topic distribution for a document.\n",
        "test_doc = \"My cat loves to play in the garden with flowers\"\n",
        "test_doc_processed = preprocess_text(test_doc)\n",
        "test_doc_bow = dictionary.doc2bow(test_doc_processed)\n",
        "topic_distribution = lda_model.get_document_topics(test_doc_bow)\n",
        "print(\"\\nTopic Distribution for test document:\")\n",
        "print(topic_distribution)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPfCO8ByiWA3",
        "outputId": "fb823fac-8cb4-42ee-e14e-31d0e27577b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyLDAvis in c:\\python312\\lib\\site-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.24.2 in c:\\users\\kabir\\appdata\\roaming\\python\\python312\\site-packages (from pyLDAvis) (1.26.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\kabir\\appdata\\roaming\\python\\python312\\site-packages (from pyLDAvis) (1.13.1)\n",
            "Requirement already satisfied: pandas>=2.0.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (1.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\python312\\lib\\site-packages (from pyLDAvis) (3.1.5)\n",
            "Requirement already satisfied: numexpr in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.10.2)\n",
            "Requirement already satisfied: funcy in c:\\python312\\lib\\site-packages (from pyLDAvis) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\python312\\lib\\site-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: gensim in c:\\users\\kabir\\appdata\\roaming\\python\\python312\\site-packages (from pyLDAvis) (4.3.3)\n",
            "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from pyLDAvis) (75.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\python312\\lib\\site-packages (from pandas>=2.0.0->pyLDAvis) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python312\\lib\\site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\kabir\\appdata\\roaming\\python\\python312\\site-packages (from gensim->pyLDAvis) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python312\\lib\\site-packages (from jinja2->pyLDAvis) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in c:\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
            "Requirement already satisfied: wrapt in c:\\users\\kabir\\appdata\\roaming\\python\\python312\\site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (1.17.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\kabir\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\kabir\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\kabir\\AppData\\Roaming\\Python\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ip (c:\\Python312\\Lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~~p (c:\\Python312\\Lib\\site-packages)\n"
          ]
        }
      ],
      "source": [
        "pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "id": "fsSL2uKciLd4",
        "outputId": "f406cecb-1fe1-42dc-dd21-ab300b6ddefc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
              "\n",
              "\n",
              "<div id=\"ldavis_el2982829959988926249970936122\" style=\"background-color:white;\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "\n",
              "var ldavis_el2982829959988926249970936122_data = {\"mdsDat\": {\"x\": [0.03079311312240058, -0.09510213221336888, 0.06430901909096833], \"y\": [0.07196240079192316, -0.015129964487928903, -0.05683243630399427], \"topics\": [1, 2, 3], \"cluster\": [1, 1, 1], \"Freq\": [44.02996086582068, 32.00859659633623, 23.961442537843094]}, \"tinfo\": {\"Term\": [\"cats\", \"nature\", \"known\", \"independent\", \"peacefully\", \"coexist\", \"dogs\", \"mat\", \"sat\", \"dog\", \"played\", \"trees\", \"gardens\", \"beautiful\", \"places\", \"humans\", \"cat\", \"garden\", \"flowers\", \"love\", \"pets\", \"loyal\", \"companions\", \"spring\", \"bloom\", \"summer\", \"mouse\", \"chased\", \"humans\", \"loyal\", \"companions\", \"spring\", \"bloom\", \"love\", \"pets\", \"summer\", \"mouse\", \"chased\", \"dogs\", \"flowers\", \"cat\", \"garden\", \"cats\", \"coexist\", \"peacefully\", \"independent\", \"nature\", \"known\", \"places\", \"gardens\", \"beautiful\", \"trees\", \"played\", \"dog\", \"sat\", \"mat\", \"mat\", \"sat\", \"dog\", \"played\", \"trees\", \"gardens\", \"beautiful\", \"places\", \"garden\", \"cat\", \"flowers\", \"coexist\", \"known\", \"independent\", \"nature\", \"peacefully\", \"chased\", \"mouse\", \"summer\", \"spring\", \"bloom\", \"loyal\", \"companions\", \"love\", \"pets\", \"humans\", \"cats\", \"dogs\", \"nature\", \"known\", \"independent\", \"peacefully\", \"coexist\", \"cats\", \"dogs\", \"places\", \"beautiful\", \"gardens\", \"trees\", \"played\", \"dog\", \"sat\", \"mat\", \"pets\", \"love\", \"companions\", \"loyal\", \"summer\", \"chased\", \"mouse\", \"bloom\", \"spring\", \"flowers\", \"garden\", \"cat\", \"humans\"], \"Freq\": [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.409329350376216, 0.8048627752410799, 0.8048567522657162, 0.8048295898278014, 0.8048255154621142, 0.8048035493166701, 0.8047951053414053, 0.8048111666090418, 0.8046670875905376, 0.8046568131031525, 1.4092538860378356, 0.7989182166546632, 0.7950733735678333, 0.7950691811045899, 0.777391925558462, 0.20182019091362335, 0.20179919907301752, 0.2017464685141961, 0.20174327988018004, 0.20174289606312254, 0.2017419217582843, 0.20173953028277222, 0.20173747833773412, 0.201733492545214, 0.2017156598142352, 0.20171005018031804, 0.20170866253403327, 0.20170238860136275, 0.7532933793928246, 0.7532841930421064, 0.7532817032835005, 0.7532640174120243, 0.7532150807773573, 0.7532070105253245, 0.7532058085728941, 0.7531985110045665, 0.7631467278561607, 0.7631461268799454, 0.7594988022298403, 0.18875302137864453, 0.1887453589319006, 0.18874486527286666, 0.188739306242876, 0.18874072282966897, 0.1891266139501421, 0.18912124809107772, 0.18897911721618074, 0.18897392306460642, 0.18897151915974558, 0.18888989371165843, 0.18888665273278354, 0.18879689264235477, 0.1887965277639384, 0.18883333755711998, 0.18910622368569752, 0.1891468968974054, 0.6607544974154144, 0.6607494201173145, 0.6607469778726589, 0.660707259262207, 0.6606792377182635, 1.1799972415150566, 0.6615741275757598, 0.1655951616846201, 0.16559238202458443, 0.16558960236454875, 0.16558754573747034, 0.1655592028455459, 0.16554832521638896, 0.1655472808354507, 0.16554435656882358, 0.16595216322411505, 0.16594494896194154, 0.16582253144857856, 0.1658147869622364, 0.16577877992096504, 0.16577603239572747, 0.16577227262434974, 0.1657736704880671, 0.16576830397678435, 0.1660533878381309, 0.16601109844383094, 0.16600822237940097, 0.16587044443254606], \"Total\": [2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.7640331323658822, 1.1595674559149747, 1.1595659364470783, 1.1595718168691922, 1.1595707051099269, 1.1595453909209663, 1.1595437963294588, 1.1595690637461877, 1.159560608305965, 1.1595594594490222, 2.259974910511001, 1.7244704067226346, 1.7242277228271796, 1.7242270074045816, 2.146495390759216, 1.0512524500105314, 1.0512471811648934, 1.0512383116597217, 1.0512370835384703, 1.0512376751123376, 1.1205355944474709, 1.1205361431726455, 1.1205356689352126, 1.1205361190600416, 1.1205388800718055, 1.1205400786802076, 1.1205401364115903, 1.1205401245630109, 1.1205401245630109, 1.1205401364115903, 1.1205400786802076, 1.1205388800718055, 1.1205361190600416, 1.1205361431726455, 1.1205356689352126, 1.1205355944474709, 1.7242270074045816, 1.7242277228271796, 1.7244704067226346, 1.0512524500105314, 1.0512376751123376, 1.0512383116597217, 1.0512370835384703, 1.0512471811648934, 1.1595594594490222, 1.159560608305965, 1.1595690637461877, 1.1595718168691922, 1.1595707051099269, 1.1595674559149747, 1.1595659364470783, 1.1595453909209663, 1.1595437963294588, 1.7640331323658822, 2.146495390759216, 2.259974910511001, 1.0512370835384703, 1.0512376751123376, 1.0512383116597217, 1.0512471811648934, 1.0512524500105314, 2.146495390759216, 2.259974910511001, 1.1205355944474709, 1.1205356689352126, 1.1205361431726455, 1.1205361190600416, 1.1205388800718055, 1.1205400786802076, 1.1205401364115903, 1.1205401245630109, 1.1595437963294588, 1.1595453909209663, 1.1595659364470783, 1.1595674559149747, 1.1595690637461877, 1.1595594594490222, 1.159560608305965, 1.1595707051099269, 1.1595718168691922, 1.7244704067226346, 1.7242270074045816, 1.7242277228271796, 1.7640331323658822], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"logprob\": [28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -2.4201, -2.9803, -2.9803, -2.9803, -2.9803, -2.9804, -2.9804, -2.9804, -2.9805, -2.9806, -2.4202, -2.9877, -2.9925, -2.9925, -3.015, -4.3636, -4.3637, -4.364, -4.364, -4.364, -4.364, -4.364, -4.364, -4.364, -4.3641, -4.3641, -4.3641, -4.3642, -2.7277, -2.7277, -2.7277, -2.7277, -2.7278, -2.7278, -2.7278, -2.7278, -2.7147, -2.7147, -2.7194, -4.1117, -4.1117, -4.1117, -4.1117, -4.1117, -4.1097, -4.1097, -4.1105, -4.1105, -4.1105, -4.1109, -4.111, -4.1114, -4.1114, -4.1112, -4.1098, -4.1096, -2.5692, -2.5692, -2.5692, -2.5692, -2.5693, -1.9893, -2.5679, -3.953, -3.953, -3.953, -3.953, -3.9532, -3.9533, -3.9533, -3.9533, -3.9509, -3.9509, -3.9516, -3.9517, -3.9519, -3.9519, -3.9519, -3.9519, -3.952, -3.9502, -3.9505, -3.9505, -3.9513], \"loglift\": [28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.5958, 0.4552, 0.4552, 0.4551, 0.4551, 0.4551, 0.4551, 0.4551, 0.4549, 0.4549, 0.348, 0.0509, 0.0462, 0.0462, -0.1953, -0.8301, -0.8302, -0.8304, -0.8304, -0.8304, -0.8943, -0.8943, -0.8943, -0.8943, -0.8944, -0.8944, -0.8944, -0.8945, 0.7421, 0.742, 0.742, 0.742, 0.742, 0.7419, 0.7419, 0.7419, 0.3241, 0.3241, 0.3191, -0.5781, -0.5782, -0.5782, -0.5782, -0.5782, -0.6742, -0.6742, -0.675, -0.675, -0.675, -0.6755, -0.6755, -0.6759, -0.6759, -1.0953, -1.2901, -1.3414, 0.9644, 0.9644, 0.9644, 0.9643, 0.9643, 0.8304, 0.2002, -0.4833, -0.4833, -0.4833, -0.4833, -0.4835, -0.4836, -0.4836, -0.4836, -0.5154, -0.5154, -0.5162, -0.5162, -0.5164, -0.5164, -0.5165, -0.5165, -0.5165, -0.9116, -0.9118, -0.9118, -0.9354]}, \"token.table\": {\"Topic\": [2, 1, 1, 2, 1, 3, 1, 3, 1, 2, 1, 3, 1, 2, 1, 2, 2, 1, 3, 3, 1, 1, 2, 1, 3, 3, 1, 2, 2, 2, 1, 1, 2], \"Freq\": [0.8924303150030454, 0.8623881196664074, 0.5799697956139582, 0.5799697956139582, 0.4658756800993175, 0.4658756800993175, 0.8623964832947517, 0.9512462967291844, 0.8623916661988279, 0.8924268029554268, 0.44248278834825244, 0.44248278834825244, 0.5798881767420442, 0.5798881767420442, 0.5799700362571544, 0.5799700362571544, 0.8924299373054011, 0.5668827765489994, 0.9512590902639142, 0.9512596662720805, 0.8624069465756337, 0.8623905361425779, 0.8924267664131892, 0.8623956288588729, 0.9512602015845883, 0.9512510643708875, 0.8624081325479077, 0.8924303743274606, 0.8924277575588621, 0.8924267569766781, 0.8623872928370826, 0.8623893403721272, 0.8924299565094315], \"Term\": [\"beautiful\", \"bloom\", \"cat\", \"cat\", \"cats\", \"cats\", \"chased\", \"coexist\", \"companions\", \"dog\", \"dogs\", \"dogs\", \"flowers\", \"flowers\", \"garden\", \"garden\", \"gardens\", \"humans\", \"independent\", \"known\", \"love\", \"loyal\", \"mat\", \"mouse\", \"nature\", \"peacefully\", \"pets\", \"places\", \"played\", \"sat\", \"spring\", \"summer\", \"trees\"]}, \"R\": 28, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 3]};\n",
              "\n",
              "function LDAvis_load_lib(url, callback){\n",
              "  var s = document.createElement('script');\n",
              "  s.src = url;\n",
              "  s.async = true;\n",
              "  s.onreadystatechange = s.onload = callback;\n",
              "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
              "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "}\n",
              "\n",
              "if(typeof(LDAvis) !== \"undefined\"){\n",
              "   // already loaded: just create the visualization\n",
              "   !function(LDAvis){\n",
              "       new LDAvis(\"#\" + \"ldavis_el2982829959988926249970936122\", ldavis_el2982829959988926249970936122_data);\n",
              "   }(LDAvis);\n",
              "}else if(typeof define === \"function\" && define.amd){\n",
              "   // require.js is available: use it to load d3/LDAvis\n",
              "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
              "   require([\"d3\"], function(d3){\n",
              "      window.d3 = d3;\n",
              "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "        new LDAvis(\"#\" + \"ldavis_el2982829959988926249970936122\", ldavis_el2982829959988926249970936122_data);\n",
              "      });\n",
              "    });\n",
              "}else{\n",
              "    // require.js not available: dynamically load d3 & LDAvis\n",
              "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
              "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
              "                 new LDAvis(\"#\" + \"ldavis_el2982829959988926249970936122\", ldavis_el2982829959988926249970936122_data);\n",
              "            })\n",
              "         });\n",
              "}\n",
              "</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "\n",
        "pyLDAvis.enable_notebook()  # For inline visualization in Google Colab\n",
        "\n",
        "# Prepare visualization\n",
        "lda_vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
        "\n",
        "# Display the visualization\n",
        "pyLDAvis.display(lda_vis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0jBzJaXTozU"
      },
      "source": [
        "## Summary\n",
        "\n",
        "For basic text preprocessing in Python, including punctuation removal and lowercasing, you can efficiently use built-in string methods and regular expressions. NLTK is well-suited for more complex linguistic preprocessing, offering tools for tokenization, stop word removal, stemming, and lemmatization. spaCy excels in providing fast and accurate pre-trained models for tokenization, lemmatization, stop word removal, and other advanced NLP tasks. Gensim is particularly useful for topic modeling and word embedding generation. For building efficient NLP pipelines, you can integrate scikit-learn's feature extraction tools with either spaCy or Gensim, leveraging their respective strengths in preprocessing and vectorization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "785XMVuKhWgP"
      },
      "source": [
        "## Your Homework\n",
        "\n",
        "Practice the sklearn CountVectorizer() and TfidfVectorizer() with your self-defined corpus. Submit your notebook to BrightSpace by 4/13 11:59 pm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Artificial intelligence is transforming industries worldwide.\",\n",
        "    \"Machine learning and deep learning are subsets of AI.\",\n",
        "    \"Natural language processing enables machines to understand human language.\",\n",
        "    \"AI applications include image recognition, speech processing, and autonomous vehicles.\",\n",
        "    \"Ethical considerations are crucial in the development of AI technologies.\",\n",
        "    \"Data is the backbone of machine learning models.\",\n",
        "    \"Supervised learning requires labeled data for training.\",\n",
        "    \"Unsupervised learning helps in discovering hidden patterns in data.\",\n",
        "    \"Reinforcement learning is inspired by behavioral psychology.\",\n",
        "    \"AI is revolutionizing healthcare, finance, and education.\"\n",
        "] \n",
        "#generated by ChatGPT text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['ai', 'and', 'applications', 'are', 'artificial', 'autonomous',\n",
              "       'backbone', 'behavioral', 'by', 'considerations', 'crucial',\n",
              "       'data', 'deep', 'development', 'discovering', 'education',\n",
              "       'enables', 'ethical', 'finance', 'for', 'healthcare', 'helps',\n",
              "       'hidden', 'human', 'image', 'in', 'include', 'industries',\n",
              "       'inspired', 'intelligence', 'is', 'labeled', 'language',\n",
              "       'learning', 'machine', 'machines', 'models', 'natural', 'of',\n",
              "       'patterns', 'processing', 'psychology', 'recognition',\n",
              "       'reinforcement', 'requires', 'revolutionizing', 'speech',\n",
              "       'subsets', 'supervised', 'technologies', 'the', 'to', 'training',\n",
              "       'transforming', 'understand', 'unsupervised', 'vehicles',\n",
              "       'worldwide'], dtype=object)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "vectorizer.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Matrix:\n",
            "  (0, 2)\t0.4472135954999579\n",
            "  (0, 24)\t0.4472135954999579\n",
            "  (0, 44)\t0.4472135954999579\n",
            "  (0, 22)\t0.4472135954999579\n",
            "  (0, 48)\t0.4472135954999579\n",
            "  (1, 28)\t0.39763979707168545\n",
            "  (1, 27)\t0.5555327633956749\n",
            "  (1, 9)\t0.4677612498987575\n",
            "  (1, 40)\t0.4677612498987575\n",
            "  (1, 0)\t0.3092972142859651\n",
            "  (2, 31)\t0.3207063470844535\n",
            "  (2, 26)\t0.641412694168907\n",
            "  (2, 33)\t0.2726296947467652\n",
            "  (2, 13)\t0.3207063470844535\n",
            "  (2, 29)\t0.3207063470844535\n",
            "  (2, 45)\t0.3207063470844535\n",
            "  (2, 19)\t0.3207063470844535\n",
            "  (3, 0)\t0.2314781013881342\n",
            "  (3, 33)\t0.2975937092579673\n",
            "  (3, 1)\t0.3500726195658407\n",
            "  (3, 21)\t0.3500726195658407\n",
            "  (3, 20)\t0.3500726195658407\n",
            "  (3, 35)\t0.3500726195658407\n",
            "  (3, 39)\t0.3500726195658407\n",
            "  (3, 3)\t0.3500726195658407\n",
            "  :\t:\n",
            "  (5, 4)\t0.5249787187068014\n",
            "  (5, 30)\t0.5249787187068014\n",
            "  (6, 27)\t0.26810347016766056\n",
            "  (6, 8)\t0.3357855442949281\n",
            "  (6, 41)\t0.45148881423757814\n",
            "  (6, 37)\t0.45148881423757814\n",
            "  (6, 25)\t0.45148881423757814\n",
            "  (6, 43)\t0.45148881423757814\n",
            "  (7, 27)\t0.24435299292358323\n",
            "  (7, 8)\t0.30603931637896886\n",
            "  (7, 46)\t0.4114927828479087\n",
            "  (7, 17)\t0.4114927828479087\n",
            "  (7, 11)\t0.4114927828479087\n",
            "  (7, 18)\t0.4114927828479087\n",
            "  (7, 32)\t0.4114927828479087\n",
            "  (8, 27)\t0.28462949616584354\n",
            "  (8, 36)\t0.47931880046384007\n",
            "  (8, 23)\t0.47931880046384007\n",
            "  (8, 5)\t0.47931880046384007\n",
            "  (8, 34)\t0.47931880046384007\n",
            "  (9, 0)\t0.31390346751909115\n",
            "  (9, 38)\t0.4747274515654985\n",
            "  (9, 16)\t0.4747274515654985\n",
            "  (9, 15)\t0.4747274515654985\n",
            "  (9, 12)\t0.4747274515654985\n",
            "\n",
            "Feature Names:\n",
            "['ai' 'applications' 'artificial' 'autonomous' 'backbone' 'behavioral'\n",
            " 'considerations' 'crucial' 'data' 'deep' 'development' 'discovering'\n",
            " 'education' 'enables' 'ethical' 'finance' 'healthcare' 'helps' 'hidden'\n",
            " 'human' 'image' 'include' 'industries' 'inspired' 'intelligence'\n",
            " 'labeled' 'language' 'learning' 'machine' 'machines' 'models' 'natural'\n",
            " 'patterns' 'processing' 'psychology' 'recognition' 'reinforcement'\n",
            " 'requires' 'revolutionizing' 'speech' 'subsets' 'supervised'\n",
            " 'technologies' 'training' 'transforming' 'understand' 'unsupervised'\n",
            " 'vehicles' 'worldwide']\n",
            "\n",
            "TF-IDF Dense Array:\n",
            "[[0.         0.         0.4472136  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.4472136  0.\n",
            "  0.4472136  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.4472136  0.         0.         0.\n",
            "  0.4472136 ]\n",
            " [0.30929721 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.46776125 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.55553276 0.3976398  0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.46776125 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.32070635 0.         0.         0.         0.\n",
            "  0.         0.32070635 0.         0.         0.         0.\n",
            "  0.         0.         0.64141269 0.         0.         0.32070635\n",
            "  0.         0.32070635 0.         0.27262969 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.32070635 0.         0.\n",
            "  0.        ]\n",
            " [0.2314781  0.35007262 0.         0.35007262 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.35007262 0.35007262 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.29759371 0.         0.35007262\n",
            "  0.         0.         0.         0.35007262 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.35007262\n",
            "  0.        ]\n",
            " [0.28357189 0.         0.         0.         0.         0.\n",
            "  0.42885592 0.42885592 0.         0.         0.42885592 0.\n",
            "  0.         0.         0.42885592 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.42885592 0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.52497872 0.\n",
            "  0.         0.         0.39044215 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.31174331 0.44627987 0.\n",
            "  0.52497872 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.33578554 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.45148881 0.         0.26810347 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.45148881 0.         0.         0.         0.45148881\n",
            "  0.         0.45148881 0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.30603932 0.         0.         0.41149278\n",
            "  0.         0.         0.         0.         0.         0.41149278\n",
            "  0.41149278 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24435299 0.         0.\n",
            "  0.         0.         0.41149278 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.41149278 0.\n",
            "  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.4793188\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.4793188\n",
            "  0.         0.         0.         0.2846295  0.         0.\n",
            "  0.         0.         0.         0.         0.4793188  0.\n",
            "  0.4793188  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [0.31390347 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.47472745 0.         0.         0.47472745 0.47472745 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.47472745 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text document using built-in Python tools.\"\"\"\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuation using regex\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) #keep word characters and whitespace\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "# Preprocess the corpus\n",
        "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Vectorization using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(preprocessed_corpus)\n",
        "\n",
        "# Print the TF-IDF matrix (sparse matrix)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix)\n",
        "\n",
        "# Print the feature names (words)\n",
        "print(\"\\nFeature Names:\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# If you want to see the dense array representation, use toarray()\n",
        "print(\"\\nTF-IDF Dense Array:\")\n",
        "print(tfidf_matrix.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: (0, '0.091*\"ai\" + 0.062*\"learning\" + 0.036*\"machine\" + 0.036*\"applications\" + 0.036*\"vehicles\" + 0.036*\"autonomous\" + 0.036*\"include\" + 0.036*\"recognition\" + 0.036*\"image\" + 0.036*\"speech\"')\n",
            "Topic 1: (1, '0.088*\"learning\" + 0.068*\"data\" + 0.027*\"hidden\" + 0.027*\"unsupervised\" + 0.027*\"helps\" + 0.027*\"discovering\" + 0.027*\"patterns\" + 0.027*\"supervised\" + 0.027*\"requires\" + 0.027*\"training\"')\n",
            "Topic 2: (2, '0.096*\"language\" + 0.055*\"processing\" + 0.055*\"human\" + 0.055*\"machines\" + 0.055*\"natural\" + 0.055*\"enables\" + 0.055*\"understand\" + 0.014*\"machine\" + 0.014*\"ai\" + 0.014*\"learning\"')\n",
            "\n",
            "Topic Distribution for test document:\n",
            "[(0, 0.33333334), (1, 0.33333334), (2, 0.33333334)]\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocesses a single text document.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Preprocess the corpus\n",
        "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Create a dictionary from the processed corpus\n",
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "\n",
        "# Create a bag-of-words representation of the corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in enumerate(lda_model.print_topics(num_topics=-1)):\n",
        "    print('Topic {}: {}'.format(idx, topic))\n",
        "\n",
        "# Get topic distribution for a document.\n",
        "test_doc = \"My cat loves to play in the garden with flowers\"\n",
        "test_doc_processed = preprocess_text(test_doc)\n",
        "test_doc_bow = dictionary.doc2bow(test_doc_processed)\n",
        "topic_distribution = lda_model.get_document_topics(test_doc_bow)\n",
        "print(\"\\nTopic Distribution for test document:\")\n",
        "print(topic_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Topic 0: (0, '0.040*\"learning\" + 0.040*\"crucial\" + 0.040*\"development\" + 0.040*\"ethical\" + 0.040*\"considerations\" + 0.040*\"technologies\" + 0.040*\"transforming\" + 0.040*\"industries\" + 0.040*\"worldwide\" + 0.040*\"artificial\"')\n",
            "Topic 1: (1, '0.135*\"learning\" + 0.084*\"data\" + 0.058*\"machine\" + 0.034*\"helps\" + 0.034*\"discovering\" + 0.034*\"hidden\" + 0.034*\"unsupervised\" + 0.034*\"patterns\" + 0.034*\"requires\" + 0.034*\"training\"')\n",
            "Topic 2: (2, '0.061*\"ai\" + 0.061*\"language\" + 0.061*\"processing\" + 0.035*\"enables\" + 0.035*\"natural\" + 0.035*\"understand\" + 0.035*\"machines\" + 0.035*\"human\" + 0.035*\"recognition\" + 0.035*\"autonomous\"')\n",
            "\n",
            "Topic Distribution for test document:\n",
            "[(0, 0.33333334), (1, 0.33333334), (2, 0.33333334)]\n"
          ]
        }
      ],
      "source": [
        "processed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "\n",
        "# Create a dictionary from the processed corpus\n",
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "\n",
        "# Create a bag-of-words representation of the corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_corpus]\n",
        "\n",
        "# Train the LDA model\n",
        "lda_model = LdaModel(bow_corpus, num_topics=3, id2word=dictionary, passes=15)\n",
        "\n",
        "# Print the topics\n",
        "for idx, topic in enumerate(lda_model.print_topics(num_topics=-1)):\n",
        "    print('Topic {}: {}'.format(idx, topic))\n",
        "\n",
        "# Get topic distribution for a document.\n",
        "test_doc = \"My cat loves to play in the garden with flowers\"\n",
        "test_doc_processed = preprocess_text(test_doc)\n",
        "test_doc_bow = dictionary.doc2bow(test_doc_processed)\n",
        "topic_distribution = lda_model.get_document_topics(test_doc_bow)\n",
        "print(\"\\nTopic Distribution for test document:\")\n",
        "print(topic_distribution)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
